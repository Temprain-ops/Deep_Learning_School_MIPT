{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"audio_homework.ipynb","provenance":[{"file_id":"1rm3tO4sYf5Xho5iTExiAOPdwzXzuFkU2","timestamp":1608455764460}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zahzrEdRCaxV"},"source":["### Spoken Language Processing\n","Homework 1\n","1. Ссылка на занятие: https://www.twitch.tv/deeplearningschool\n","2. Материалы: https://vk.cc/bVLCOC https://vk.cc/bVLD3Z\n","\n","В этом задании предлагается обучить классификатор класса возраста по голосу (пример с тем, как это можно сделать для пола см. в семинаре)\n","\n","P.S. не забудьте, что если то вы работает в Colab, то вы можете поменять среду выполнения на GPU/TPU!\n","\n","Вопросы по заданию/материалам: @Nestyme"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wSgHrbiEc8x","executionInfo":{"status":"ok","timestamp":1609077909425,"user_tz":-180,"elapsed":6846,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}},"outputId":"27f87591-bd0d-40b4-ef11-43c98df81ea5"},"source":["!pip3 install timit-utils==0.9.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting timit-utils==0.9.0\n","  Downloading https://files.pythonhosted.org/packages/22/32/0c98f7f44386947b9e4080f54f09a7380c390e0b8337ab0b87050d49c43a/timit_utils-0.9.0-py3-none-any.whl\n","Collecting python-speech-features\n","  Downloading https://files.pythonhosted.org/packages/ff/d1/94c59e20a2631985fbd2124c45177abaa9e0a4eee8ba8a305aa26fc02a8e/python_speech_features-0.6.tar.gz\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from timit-utils==0.9.0) (3.2.2)\n","Collecting SoundFile>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from timit-utils==0.9.0) (1.1.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from timit-utils==0.9.0) (1.19.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from timit-utils==0.9.0) (1.4.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->timit-utils==0.9.0) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->timit-utils==0.9.0) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->timit-utils==0.9.0) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->timit-utils==0.9.0) (2.4.7)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from SoundFile>=0.8.0->timit-utils==0.9.0) (1.14.4)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->timit-utils==0.9.0) (2018.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->timit-utils==0.9.0) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->SoundFile>=0.8.0->timit-utils==0.9.0) (2.20)\n","Building wheels for collected packages: python-speech-features\n","  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-speech-features: filename=python_speech_features-0.6-cp36-none-any.whl size=5890 sha256=b23ea0e23313d68a01716789c4ece63c1814ddf4ffa5aae0e41cdecb5010117f\n","  Stored in directory: /root/.cache/pip/wheels/3c/42/7c/f60e9d1b40015cd69b213ad90f7c18a9264cd745b9888134be\n","Successfully built python-speech-features\n","Installing collected packages: python-speech-features, SoundFile, timit-utils\n","Successfully installed SoundFile-0.10.3.post1 python-speech-features-0.6 timit-utils-0.9.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PveQyMmsErXM","executionInfo":{"status":"ok","timestamp":1609078003916,"user_tz":-180,"elapsed":101311,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}},"outputId":"5e0a3188-1400-4935-baef-23396fabd187"},"source":["!pip3 install torchaudio"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting torchaudio\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/f9/618434cf4e46dc975871e1516f5499abef6564ab4366f9b2321ee536be14/torchaudio-0.7.2-cp36-cp36m-manylinux1_x86_64.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 3.6MB/s \n","\u001b[?25hCollecting torch==1.7.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/4f/acf48b3a18a8f9223c6616647f0a011a5713a985336088d7c76f3a211374/torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8MB)\n","\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (1.19.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (3.7.4.3)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (0.8)\n","\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n","Installing collected packages: torch, torchaudio\n","  Found existing installation: torch 1.7.0+cu101\n","    Uninstalling torch-1.7.0+cu101:\n","      Successfully uninstalled torch-1.7.0+cu101\n","Successfully installed torch-1.7.1 torchaudio-0.7.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifZp0NvTEo8V","executionInfo":{"status":"ok","timestamp":1609078019702,"user_tz":-180,"elapsed":117081,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}},"outputId":"cc12df8c-665f-427a-e6b8-c7f2f7172537"},"source":["! wget https://ndownloader.figshare.com/files/10256148 "],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2020-12-27 14:06:44--  https://ndownloader.figshare.com/files/10256148\n","Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 34.255.82.212, 52.16.75.60, 3.248.141.196, ...\n","Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|34.255.82.212|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/10256148/TIMIT.zip [following]\n","--2020-12-27 14:06:44--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/10256148/TIMIT.zip\n","Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.40.131\n","Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.40.131|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 440207227 (420M) [binary/octet-stream]\n","Saving to: ‘10256148’\n","\n","10256148            100%[===================>] 419.81M  30.1MB/s    in 15s     \n","\n","2020-12-27 14:06:59 (28.5 MB/s) - ‘10256148’ saved [440207227/440207227]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W401P4FhEq0c","executionInfo":{"status":"ok","timestamp":1609078030139,"user_tz":-180,"elapsed":127511,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}}},"source":["!unzip -q 10256148"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"u0bovLZ0Ew5V","executionInfo":{"status":"ok","timestamp":1609078033238,"user_tz":-180,"elapsed":130603,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}}},"source":["import timit_utils as tu\n","import os\n","import librosa\n","import numpy as np\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score\n","\n","import IPython\n","_TIMIT_PATH = 'data/lisa/data/timit/raw/TIMIT'"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gd-qfC9-DdnJ"},"source":["## Задание 1\n","Загрузите данные для обучения. Для этого:\n","1. Скачайте датасет TIMIT (см семинар)\n","2. Соберите пары \"голос\"  — \"класс возраста\" также, как на семинаре собирались пары \"голос\"  — \"пол\". Аудиодорожки сконвертируйте в мелспектрограммы при помощи `torchaudio либо` `librosa`\n","\n","P.S. вы можете использовать свою реализацию, а можете предложенную (см следующие ячейки)"]},{"cell_type":"code","metadata":{"id":"DhPyP4T5DdAD","executionInfo":{"status":"ok","timestamp":1609078033246,"user_tz":-180,"elapsed":130605,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}}},"source":["import timit_utils as tu\n","import os\n","import librosa\n","import numpy as np\n","from tqdm import tqdm\n","import torch as t\n","\n","\n","class timit_dataloader:\n","    def __init__(self, data_path=_TIMIT_PATH, train_mode=True, age_mode=True):\n","        self.doc_file_path = os.path.join(data_path, 'DOC', 'SPKRINFO.TXT')\n","        self.corpus = tu.Corpus(data_path)\n","        with open(self.doc_file_path) as f:\n","            self.id_age_dict = dict(\n","                [(tmp.split(' ')[0], 86 - int(tmp.split('  ')[5].split('/')[-1].replace('??', '50'))) \\\n","                 for tmp in f.readlines()[39:]])\n","        if train_mode:\n","            self.trainset = self.create_dataset('train', age_mode=age_mode)\n","            self.validset = self.create_dataset('valid', age_mode=age_mode)\n","        self.testset = self.create_dataset('test', age_mode=age_mode)\n","\n","    def return_age(self, id):\n","        return self.id_age_dict[id]\n","\n","    def return_data(self):\n","        return self.trainset, self.validset, self.testset\n","\n","    def return_test(self):\n","        return self.testset\n","\n","    def create_dataset(self, mode, age_mode=False):\n","        global people\n","        assert mode in ['train', 'valid', 'test']\n","        if mode == 'train':\n","            people = [self.corpus.train.person_by_index(i) for i in range(350)]\n","        if mode == 'valid':\n","            people = [self.corpus.train.person_by_index(i) for i in range(350, 400)]\n","        if mode == 'test':\n","            people = [self.corpus.test.person_by_index(i) for i in range(150)]\n","        spectrograms_and_targets = []\n","        for person in tqdm(people):\n","              try:\n","                  target = self.return_age(person.name)\n","                  for i in range(len(person.sentences)):\n","                      spectrograms_and_targets.append(\n","                          self.preprocess_sample(person.sentence_by_index(i).raw_audio, target, age_mode=True))\n","              except:\n","                  print(person.name, target)\n","\n","        X, y = map(np.stack, zip(*spectrograms_and_targets))\n","        X = X.transpose([0, 2, 1])  # to [batch, time, channels]\n","        return X, y\n","\n","    @staticmethod\n","    def spec_to_image(spec, eps=1e-6):\n","        mean = spec.mean()\n","        std = spec.std()\n","        spec_norm = (spec - mean) / (std + eps)\n","        spec_min, spec_max = spec_norm.min(), spec_norm.max()\n","        spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n","        spec_scaled = spec_scaled.astype(np.uint8)\n","        return spec_scaled\n","\n","    @staticmethod\n","    def clasterize_by_age(age):\n","        if age <= 25:\n","            return 0\n","        if age > 25 and age <=35:\n","            return 1\n","        if age >35:\n","          return 2\n","        \n","\n","    def preprocess_sample(self, amplitudes, target, age_mode=False, sr=16000, max_length=150):\n","        spectrogram = librosa.feature.melspectrogram(amplitudes, sr=sr, n_mels=256, fmin=1, fmax=8192)[:, :max_length]\n","        spectrogram = np.pad(spectrogram, [[0, 0], [0, max(0, max_length - spectrogram.shape[1])]], mode='constant')\n","        target = self.clasterize_by_age(target)\n","        return self.spec_to_image(np.float32(spectrogram)), target\n","\n","    def preprocess_sample_inference(self, amplitudes, sr=16000, max_length=150, device='cpu'):\n","        spectrogram = librosa.feature.melspectrogram(amplitudes, sr=sr, n_mels=256, fmin=1, fmax=8192)[:, :max_length]\n","        spectrogram = np.pad(spectrogram, [[0, 0], [0, max(0, max_length - spectrogram.shape[1])]], mode='constant')\n","        spectrogram = np.array([self.spec_to_image(np.float32(spectrogram))]).transpose([0, 2, 1])\n","\n","        return t.tensor(spectrogram, dtype=t.float).to(device, non_blocking=True)\n","\n","\n","class dataloader:\n","    def __init__(self, spectrograms, targets):\n","        self.data = list(zip(spectrograms, targets))\n","\n","    def next_batch(self, batch_size, device):\n","        indices = np.random.randint(len(self.data), size=batch_size)\n","\n","        input = [self.data[i] for i in indices]\n","\n","        source = [line[0] for line in input]\n","        target = [line[1] for line in input]\n","\n","        return self.torch_batch(source, target, device)\n","\n","    @staticmethod\n","    def torch_batch(source, target, device):\n","        #print(source)\n","        #print(target)\n","        return tuple(\n","            [\n","                t.tensor(val, dtype=t.float).to(device, non_blocking=True)\n","                for val in [source, target]\n","            ]\n","        )\n","\n","    @staticmethod\n","    def padd_sequences(lines, pad_token=0):\n","        lengths = [len(line) for line in lines]\n","        max_length = max(lengths)\n","\n","        return np.array(\n","            [\n","                line + [pad_token] * (max_length - lengths[i])\n","                for i, line in enumerate(lines)\n","            ]\n","        )"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tpz1Q5VOFxLM"},"source":["Простая сверточная сеть, ее можно дотюнить или поменять по желанию"]},{"cell_type":"code","metadata":{"id":"qF9fIVq7Dbwx","executionInfo":{"status":"ok","timestamp":1609079275843,"user_tz":-180,"elapsed":871,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim.adamw\n","\n","class Model(nn.Module):\n","    def __init__(self, window_sizes=(3, 4, 5, 6)):\n","        super(Model, self).__init__()\n","\n","        self.convs = nn.ModuleList([\n","            nn.Conv2d(1, 256, [window_size, 256], padding=(window_size - 1, 0))\n","            for window_size in window_sizes\n","        ])\n","\n","        self.fc1 = nn.Linear(256 * len(window_sizes), 514)\n","        self.fc2 = nn.Linear(514,256)\n","        self.fc3 = nn.Linear(256,3)\n","\n","    def forward(self, x):\n","        # Apply a convolution + max pool layer for each window size\n","        x = torch.unsqueeze(x, 1)  # [B, C, T, E] Add a channel dim.\n","        xs = []\n","        for conv in self.convs:\n","            x2 = F.relu(conv(x))  # [B, F, T, 1]\n","            x2 = torch.squeeze(x2, -1)  # [B, F, T]\n","            x2 = F.max_pool1d(x2, x2.size(2))  # [B, F, 1]\n","            xs.append(x2)\n","        x = torch.cat(xs, 2)  # [B, F, window]\n","\n","        # FC\n","        x = x.view(x.size(0), -1)  # [B, F * window]\n","        logits1 = self.fc1(x)  # [B, class]\n","        logits2=self.fc2(logits1)\n","        probs=self.fc3(logits2)\n","        return probs\n","\n","    def loss(self, probs, targets):\n","        return nn.CrossEntropyLoss()(probs, targets)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EoOR641Fkzx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609079275844,"user_tz":-180,"elapsed":644,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}},"outputId":"d4116fa8-9c28-4411-9849-0fd2c299c14d"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'using {device} mode')\n","patience = 500\n","best_loss = 1000\n","cnt = 0\n","\n","model = Model()\n","if device == torch.device('cuda'):\n","    model.cuda()\n","else:\n","    model.cpu()\n","model.train()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["using cpu mode\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (convs): ModuleList(\n","    (0): Conv2d(1, 256, kernel_size=[3, 256], stride=(1, 1), padding=(2, 0))\n","    (1): Conv2d(1, 256, kernel_size=[4, 256], stride=(1, 1), padding=(3, 0))\n","    (2): Conv2d(1, 256, kernel_size=[5, 256], stride=(1, 1), padding=(4, 0))\n","    (3): Conv2d(1, 256, kernel_size=[6, 256], stride=(1, 1), padding=(5, 0))\n","  )\n","  (fc1): Linear(in_features=1024, out_features=514, bias=True)\n","  (fc2): Linear(in_features=514, out_features=256, bias=True)\n","  (fc3): Linear(in_features=256, out_features=3, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sLUggB9iF6s_","executionInfo":{"status":"ok","timestamp":1609079394122,"user_tz":-180,"elapsed":118712,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}},"outputId":"37021a6d-bae5-4f69-cd64-941825dcc2aa"},"source":["_timit_dataloader = timit_dataloader()\n","train, valid, test = _timit_dataloader.return_data()\n","\n","trainset = dataloader(*train)\n","validset = dataloader(*valid)\n","testset = dataloader(*test)\n","BATCH_SIZE = 64\n","\n","optimizer = torch.optim.AdamW(\n","    [p for p in model.parameters() if p.requires_grad], eps=1e-4\n",")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["100%|██████████| 350/350 [01:15<00:00,  4.66it/s]\n","100%|██████████| 50/50 [00:10<00:00,  4.70it/s]\n","100%|██████████| 150/150 [00:31<00:00,  4.76it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ScCZEMvXHkmz"},"source":["#Задание 2\n","1. Обучите свой классификатор категории возраста\n","2. Попробуйте улучшить результат. Можно попробовать усложнить сетку, подвигать границы категорий, поискать новые данные, что угодно, кроме учиться на тесте :)\n","3. Какой подход оказался самым эффективным? Как думаете, почему?\n","4. Как считаете, где можно было бы применить такой классификатор в качестве вспомогательной задачи?\n"]},{"cell_type":"code","metadata":{"id":"AvK_pwq8e2gU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609079394126,"user_tz":-180,"elapsed":118317,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}},"outputId":"9ee4754d-46ac-4e81-dddb-f39f9cebc921"},"source":["print(device)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["cpu\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uY4fOjiwgxQt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609079815258,"user_tz":-180,"elapsed":539204,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}},"outputId":"1677c5ba-9ac8-4d0c-a80f-93d5d9ff51cd"},"source":["import torch as t\r\n","from sklearn.metrics import f1_score\r\n","\r\n","for i in tqdm(range(200)):\r\n","\r\n","    optimizer.zero_grad()\r\n","    \r\n","    input, target = trainset.next_batch(BATCH_SIZE, device=device)\r\n","    out = model(input)\r\n","    \r\n","    loss = model.loss(out, target.type(torch.LongTensor))\r\n","    loss.backward()\r\n","    optimizer.step()\r\n","\r\n","    if i % 50 == 0:\r\n","        model.eval()\r\n","\r\n","        with torch.no_grad():\r\n","            optimizer.zero_grad()\r\n","\r\n","            input, target = validset.next_batch(BATCH_SIZE, device=device)\r\n","            out = model(input)\r\n","            valid_loss = model.loss(out, target.type(torch.LongTensor))\r\n","            out, target = out.cpu().argmax(1).detach().numpy(), target.cpu().detach().numpy()\r\n","            \r\n","            print(f'f1_score:{f1_score(target,out,average=None)}')\r\n","            print(\"i {}, valid {}\".format(i, valid_loss.item()))\r\n","            print(\"_________\")\r\n","\r\n","        model.train()\r\n","\r\n","    if i % 50 == 0 and best_loss > valid_loss.item():\r\n","        best_loss = valid_loss.item()\r\n","        cnt = 0\r\n","    else:\r\n","        cnt += 1\r\n","\r\n","    if cnt > patience:\r\n","        break\r\n","print('training finished')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["  0%|          | 1/200 [00:03<11:16,  3.40s/it]"],"name":"stderr"},{"output_type":"stream","text":["f1_score:[0.         0.70707071 0.        ]\n","i 0, valid 50.966033935546875\n","_________\n"],"name":"stdout"},{"output_type":"stream","text":[" 26%|██▌       | 51/200 [01:48<06:05,  2.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["f1_score:[0.         0.62365591 0.        ]\n","i 50, valid 1.0663971900939941\n","_________\n"],"name":"stdout"},{"output_type":"stream","text":[" 50%|█████     | 101/200 [03:33<04:02,  2.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["f1_score:[0.22222222 0.60240964 0.11111111]\n","i 100, valid 1.1418808698654175\n","_________\n"],"name":"stdout"},{"output_type":"stream","text":[" 76%|███████▌  | 151/200 [05:19<02:00,  2.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["f1_score:[0.32258065 0.65909091 0.        ]\n","i 150, valid 0.9994719624519348\n","_________\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 200/200 [07:00<00:00,  2.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["training finished\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"S4bruW2QVTnk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609079816848,"user_tz":-180,"elapsed":540167,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}},"outputId":"aa41f070-077a-4778-b60b-0f594e2db1fa"},"source":["model.eval()\r\n","input,target=testset.next_batch(BATCH_SIZE,device=device)\r\n","out = model(input)\r\n","out, target = out.cpu().argmax(1).detach().numpy(), target.cpu().detach().numpy()\r\n","\r\n","\r\n","print(f1_score(target,out,average=None))\r\n","print(f1_score(target,out,average=\"weighted\"))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[0.51282051 0.69333333 0.28571429]\n","0.5675915750915752\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TKZYNz3ud--1"},"source":["Вывод: Использован оптимайзер AdamW, CrossEntropyLoss, добавлен 1 Conv2d и 2 fc-слоя, немного измененены границы категорий, чтобы примерно сбалансировать классы. Как подзадача может использоваться в голосовой авторизации пользователей и т.д."]},{"cell_type":"code","metadata":{"id":"ZYTbk3MBgDQN","executionInfo":{"status":"ok","timestamp":1609078573842,"user_tz":-180,"elapsed":671117,"user":{"displayName":"Илья Волынец","photoUrl":"","userId":"08811045169562879935"}}},"source":[""],"execution_count":12,"outputs":[]}]}