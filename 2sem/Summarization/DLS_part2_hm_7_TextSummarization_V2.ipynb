{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DLS_part2_hm_7_TextSummarization V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b3b2853280ff4718ae63723c27347c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5cc33efa05894ec88be9bd471188e69d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4ffe091214564bc183747f62e2effeb4",
              "IPY_MODEL_bd921d4a30ce419c80d8f41557f0bc03"
            ]
          }
        },
        "5cc33efa05894ec88be9bd471188e69d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ffe091214564bc183747f62e2effeb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_30b850e3c0e043a59b4ce8928a1363a1",
            "_dom_classes": [],
            "description": "Train Loss: 0.182, Valid Loss: 0.178:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 552,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1287e4d6cfc546fcab3b3175dbcc9ee6"
          }
        },
        "bd921d4a30ce419c80d8f41557f0bc03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_03cde1d8acbf452591ef3f4227420c47",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/552 [00:00&lt;03:41,  2.49it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4ee89ee09434de496f31d4b028b166a"
          }
        },
        "30b850e3c0e043a59b4ce8928a1363a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1287e4d6cfc546fcab3b3175dbcc9ee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03cde1d8acbf452591ef3f4227420c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4ee89ee09434de496f31d4b028b166a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e26339fb20d04ca8961fc85487525efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9bb50fbb67274c34ac5d99dc8677b84a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e6e661e67f86460989add87e1496b586",
              "IPY_MODEL_6f8b68b5bcf94ab3883a3886e6bf0e9a"
            ]
          }
        },
        "9bb50fbb67274c34ac5d99dc8677b84a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6e661e67f86460989add87e1496b586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_08f8e13637584881b3f43d7fc287b5cc",
            "_dom_classes": [],
            "description": "Epoch: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed8c001eb0fe46aeaf3762b99ad415e4"
          }
        },
        "6f8b68b5bcf94ab3883a3886e6bf0e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ac590b88ba1d46d6938542d448420ab2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [11:56&lt;00:00, 358.29s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_456b8b1ea6da446a9f8e3c9d8f8e0962"
          }
        },
        "08f8e13637584881b3f43d7fc287b5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed8c001eb0fe46aeaf3762b99ad415e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac590b88ba1d46d6938542d448420ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "456b8b1ea6da446a9f8e3c9d8f8e0962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4c68f466bb245b2a2c7fded0c84c722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5cdb9298c6a44326bb6026a7ac74edf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33b4d29d715d45afaf342b8c12c2c092",
              "IPY_MODEL_5d6996bd96a244e386d23d646b9f71d4"
            ]
          }
        },
        "5cdb9298c6a44326bb6026a7ac74edf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33b4d29d715d45afaf342b8c12c2c092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5dbb21716336492ebb7756cfbb27827e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 91,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 91,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47a2f3c7fbe14d11ab7339f8bb6e6c8e"
          }
        },
        "5d6996bd96a244e386d23d646b9f71d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_93f6147a5563491993fd8f46a7ac3fb5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 91/91 [01:00&lt;00:00,  1.92it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7c503b680bc4206862672041d6a0872"
          }
        },
        "5dbb21716336492ebb7756cfbb27827e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47a2f3c7fbe14d11ab7339f8bb6e6c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93f6147a5563491993fd8f46a7ac3fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7c503b680bc4206862672041d6a0872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmb8UhIzOnfK"
      },
      "source": [
        "# Text Summarization. Homework\n",
        "\n",
        "Всем привет! Это домашка по суммаризации текста.\n",
        "\n",
        "На семинаре мы рассмотрели базовые модели для суммаризации текста. Попробуйте теперь улучшить два метода: TextRank и Extractive RNN. Задание достаточно большое и требует хорошую фантазию, тут можно эксперементировать во всю.\n",
        "\n",
        "Для сдачи заданий надо получить определенное качество по test-у:\n",
        "\n",
        "- 1 задание: 0.35 BLEU\n",
        "- 2 задание: 0.35 BLEU\n",
        "\n",
        "Если ваш подход пробивает это качество – задание считается пройденным. Плюсом будет описание того, почему вы решили использовать то или иное решение. \n",
        "\n",
        "Датасет: gazeta.ru\n",
        "\n",
        "**P.S.** Возможно, в датасете находятся пустые данные. Проверьте эту гипотезу, и если надо, сделайте предобратоку датасета.\n",
        "\n",
        "\n",
        "`Ноутбук создан на основе семинара Гусева Ильи на кафедре компьютерной лингвистики МФТИ.`\n",
        "\n",
        "Загрузим датасет и необходимые библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqkLTkFRfXvA"
      },
      "source": [
        "!wget -q https://www.dropbox.com/s/43l702z5a5i2w8j/gazeta_train.txt\n",
        "!wget -q https://www.dropbox.com/s/k2egt3sug0hb185/gazeta_val.txt\n",
        "!wget -q https://www.dropbox.com/s/3gki5n5djs9w0v6/gazeta_test.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXS1sdYZCluU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a64382-67fc-4935-ace1-78cd36505233"
      },
      "source": [
        "!pip install -Uq razdel allennlp torch fasttext OpenNMT-py networkx pymorphy2 nltk rouge==0.3.1 summa\n",
        "!pip install -Uq transformers youtokentome"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 512kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 16.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 18.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 30.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 32.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 43.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 41.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.5MB 46.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.2MB 37.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 58.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 57.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.0MB 49.2MB/s \n",
            "\u001b[?25h  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: botocore 1.19.30 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7MB 12.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 32.4MB/s \n",
            "\u001b[31mERROR: allennlp 1.2.2 has requirement transformers<3.6,>=3.4, but you'll have transformers 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pZ2UGS2DGjH"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def read_gazeta_records(file_name, shuffle=True, sort_by_date=False):\n",
        "    assert shuffle != sort_by_date\n",
        "    records = []\n",
        "    with open(file_name, \"r\") as r:\n",
        "        for line in r:\n",
        "            records.append(eval(line)) # Simple hack\n",
        "    records = pd.DataFrame(records)\n",
        "    if sort_by_date:\n",
        "        records = records.sort(\"date\")\n",
        "    if shuffle:\n",
        "        records = records.sample(frac=1)\n",
        "    return records"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNDp-BunEA91"
      },
      "source": [
        "train_records = read_gazeta_records(\"gazeta_train.txt\")\n",
        "val_records = read_gazeta_records(\"gazeta_val.txt\")\n",
        "test_records = read_gazeta_records(\"gazeta_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsAcVSli3r3S"
      },
      "source": [
        "## 1 задание: TextRank (порог: 0.35 BLEU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7jAQp-_Ds98"
      },
      "source": [
        "TextRank - unsupervised метод для составления кратких выжимок из текста. \n",
        "Описание метода:\n",
        "\n",
        "1. Сплитим текст по предложениям\n",
        "2. Считаем \"похожесть\" предложений между собой\n",
        "3. Строим граф предложений с взвешенными ребрами\n",
        "4. С помощью алгоритм PageRank получаем наиболее важные предложения, на основе которых делаем summary.\n",
        "\n",
        "Функция похожести можно сделать и из нейросетевых(или около) моделек: FastText, ELMO и BERT. Выберете один метод, загрузите предобученную модель и с ее помощью для каждого предложениия сделайте sentence embedding. С помощью косинусной меры определяйте похожесть предложений.\n",
        "\n",
        "Предобученные модели можно взять по [ссылке](http://docs.deeppavlov.ai/en/master/features/pretrained_vectors.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yumPanN9_21A"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "\n",
        "\n",
        "def calc_scores(references, predictions, metric=\"all\"):\n",
        "    print(\"Count:\", len(predictions))\n",
        "    print(\"Ref:\", references[-1])\n",
        "    print(\"Hyp:\", predictions[-1])\n",
        "\n",
        "    if metric in (\"bleu\", \"all\"):\n",
        "        print(\"BLEU: \", corpus_bleu([[r] for r in references], predictions))\n",
        "    if metric in (\"rouge\", \"all\"):\n",
        "        rouge = Rouge()\n",
        "        scores = rouge.get_scores(predictions, references, avg=True)\n",
        "        print(\"ROUGE: \", scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2GwyRrMPAzS"
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pymorphy2\n",
        "import razdel\n",
        "\n",
        "\n",
        "def your_super_words_similarity(words1, words2):\n",
        "    # Your code\n",
        "    pass\n",
        "\n",
        "\n",
        "def gen_text_rank_summary(text, calc_similarity=unique_words_similarity, summary_part=0.1, lower=True, morph=None):\n",
        "    '''\n",
        "    Составление summary с помощью TextRank\n",
        "    '''\n",
        "    # Разбиваем текст на предложения\n",
        "    sentences = [sentence.text for sentence in razdel.sentenize(text)]\n",
        "    n_sentences = len(sentences)\n",
        "\n",
        "    # Токенизируем предложения\n",
        "    sentences_words = [[token.text.lower() if lower else token.text for token in razdel.tokenize(sentence)] for sentence in sentences]\n",
        "\n",
        "    # При необходимости лемматизируем слова\n",
        "    if morph is not None:\n",
        "        sentences_words = [[morph.parse(word)[0].normal_form for word in words] for words in sentences_words]\n",
        "\n",
        "    # Для каждой пары предложений считаем близость\n",
        "    pairs = combinations(range(n_sentences), 2)\n",
        "    scores = [(i, j, calc_similarity(sentences_words[i], sentences_words[j])) for i, j in pairs]\n",
        "\n",
        "    # Строим граф с рёбрами, равными близости между предложениями\n",
        "    g = nx.Graph()\n",
        "    g.add_weighted_edges_from(scores)\n",
        "\n",
        "    # Считаем PageRank\n",
        "    pr = nx.pagerank(g)\n",
        "    result = [(i, pr[i], s) for i, s in enumerate(sentences) if i in pr]\n",
        "    result.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Выбираем топ предложений\n",
        "    n_summary_sentences = max(int(n_sentences * summary_part), 1)\n",
        "    result = result[:n_summary_sentences]\n",
        "\n",
        "    # Восстанавливаем оригинальный их порядок\n",
        "    result.sort(key=lambda x: x[0])\n",
        "\n",
        "    # Восстанавливаем текст выжимки\n",
        "    predicted_summary = \" \".join([sentence for i, proba, sentence in result])\n",
        "    predicted_summary = predicted_summary.lower() if lower else predicted_summary\n",
        "    return predicted_summary\n",
        "\n",
        "def calc_text_rank_score(records, calc_similarity=unique_words_similarity, summary_part=0.1, lower=True, nrows=1000, morph=None):\n",
        "    references = []\n",
        "    predictions = []\n",
        "\n",
        "    for text, summary in records[['text', 'summary']].values[:nrows]:\n",
        "        summary = summary if not lower else summary.lower()\n",
        "        references.append(summary)\n",
        "\n",
        "        predicted_summary = gen_text_rank_summary(text, calc_similarity, summary_part, lower, morph=morph)\n",
        "        text = text if not lower else text.lower()\n",
        "        predictions.append(predicted_summary)\n",
        "\n",
        "    calc_scores(references, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EejZGUIy5zK9"
      },
      "source": [
        "calc_text_rank_score(test_records, calc_similarity=your_super_words_similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdTrfxycB7cd"
      },
      "source": [
        "## 2 Задание: Extractive RNN (порог: 0.35 BLEU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q7DeHDYFSjX"
      },
      "source": [
        "Второй метод, который вам предлагается улучшить – поиск предложений для summary с помощью RNN. В рассмотренной методе мы использовали LSTM для генерации sentence embedding. Попробуйте использовать другие архитектуры: CNN, Transformer; или добавьте предобученные модели, как и в первом задании.\n",
        "\n",
        "P.S. Тут предполагается, что придется изменять много кода в ячееках (например, поменять токенизацию). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dZamxigdEc-"
      },
      "source": [
        "### Модель\n",
        "\n",
        "Картинка для привлечения внимания:\n",
        "\n",
        "![img](https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_398421%2Fimages%2Farchitecture.png)\n",
        "\n",
        "Статья с оригинальным методом:\n",
        "https://arxiv.org/pdf/1611.04230.pdf\n",
        "\n",
        "Список вдохновения: \n",
        "- https://towardsdatascience.com/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-d2ee64b9dd0b Пример того, как можно применять CNN в текстовых задачах\n",
        "- https://arxiv.org/pdf/1808.08745.pdf Очень крутой метод генерации summary без Transformers\n",
        "- https://towardsdatascience.com/super-easy-way-to-get-sentence-embedding-using-fasttext-in-python-a70f34ac5b7c – простой метод генерации sentence embedding\n",
        "- https://towardsdatascience.com/fse-2b1ffa791cf9 – Необычный метод генерации sentence embedding\n",
        "- https://github.com/UKPLab/sentence-transformers – BERT предобученный для sentence embedding\n",
        "\n",
        "P.S. Выше написанные ссылки нужны только для разогрева вашей фантазии, можно воспользоваться ими, а можно придумать свой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOH4ZbLkg_sM"
      },
      "source": [
        "Комментарий к заданию:\n",
        "Если посмотреть на архитектуру ~~почти~~ SummaRuNNer, то в ней есть два главных элемента: первая часть, которая читает предложения и возвращает векторы на каждое предложение, и вторая, которая выбирает предложения для суммаризации. Вторую часть мы не трогаем, а первую меняем. На что меняем – как вы решите. Главное: она должна иметь хорошее качество и встроиться в текущую модель."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxsc0Orf8hGq"
      },
      "source": [
        "import copy\n",
        "import random\n",
        "\n",
        "def build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n",
        "    '''\n",
        "    Жадное построение oracle summary\n",
        "    '''\n",
        "    gold_summary = gold_summary.lower() if lower else gold_summary\n",
        "    # Делим текст на предложения\n",
        "    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
        "    n_sentences = len(sentences)\n",
        "    oracle_summary_sentences = set()\n",
        "    score = -1.0\n",
        "    summaries = []\n",
        "    for _ in range(min(n_sentences, 2)):\n",
        "        for i in range(n_sentences):\n",
        "            if i in oracle_summary_sentences:\n",
        "                continue\n",
        "            current_summary_sentences = copy.copy(oracle_summary_sentences)\n",
        "            # Добавляем какое-то предложения к уже существующему summary\n",
        "            current_summary_sentences.add(i)\n",
        "            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n",
        "            # Считаем метрики\n",
        "            current_score = calc_score(current_summary, gold_summary)\n",
        "            summaries.append((current_score, current_summary_sentences))\n",
        "        # Если получилось улучшить метрики с добавлением какого-либо предложения, то пробуем добавить ещё\n",
        "        # Иначе на этом заканчиваем\n",
        "        best_summary_score, best_summary_sentences = max(summaries)\n",
        "        if best_summary_score <= score:\n",
        "            break\n",
        "        oracle_summary_sentences = best_summary_sentences\n",
        "        score = best_summary_score\n",
        "    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n",
        "    return oracle_summary, oracle_summary_sentences\n",
        "\n",
        "def calc_single_score(pred_summary, gold_summary, rouge):\n",
        "    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T_ak-KDB8rp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654c21db-974e-4dfa-c1a9-342ef0354510"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def calc_oracle_score(records, nrows=1000, lower=True):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    rouge = Rouge()\n",
        "  \n",
        "    for text, summary in tqdm(records[['text', 'summary']].values[:nrows]):\n",
        "        summary = summary if not lower else summary.lower()\n",
        "        references.append(summary)\n",
        "        predicted_summary, _ = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge))\n",
        "        predictions.append(predicted_summary)\n",
        "\n",
        "    calc_scores(references, predictions)\n",
        "\n",
        "calc_oracle_score(test_records)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:59<00:00, 16.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Count: 1000\n",
            "Ref: врача-кардиолога из москвы лалу оганесян убил ее 60-летний муж, сообщили в столичном ск. по данным сми, 19-летний сын супругов позвонил в полицию и сообщил, что не может открыть дверь в квартиру и дозвониться до родителей. прибывшие на место происшествия правоохранители увидели труп оганесян. рядом находился ее супруг, пытавшийся покончить с собой. как пояснил сын погибшей, его отец мог пойти на преступление из ревности.\n",
            "Hyp: родители также не отвечали на звонки, в связи с чем молодой человек решил обратиться в полицию. прибывшие на место правоохранители вскрыли дверь и, попав внутрь, увидели труп оганесян с колото-резаными ранами, а также ее супруга, лежавшего в луже крови.\n",
            "BLEU:  0.43337706663257897\n",
            "ROUGE:  {'rouge-1': {'f': 0.3567839291479586, 'p': 0.43613372441109544, 'r': 0.3206579972798067}, 'rouge-2': {'f': 0.20440197194024465, 'p': 0.2568239687292027, 'r': 0.18231175967471328}, 'rouge-l': {'f': 0.30494924151735575, 'p': 0.40480572001613513, 'r': 0.29619161794718935}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9uHyligL01m"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWgjewfWrbJZ"
      },
      "source": [
        "## (!)\n",
        "Если надо, поменяйте код загрузки токенизатора"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qIRKm4TCHzN"
      },
      "source": [
        "import os\n",
        "\n",
        "import youtokentome as yttm\n",
        "\n",
        "def train_bpe(records, model_path, model_type=\"bpe\", vocab_size=30000, lower=True):\n",
        "    temp_file_name = \"temp.txt\"\n",
        "    with open(temp_file_name, \"w\") as temp:\n",
        "        for text, summary in records[['text', 'summary']].values:\n",
        "            if lower:\n",
        "                summary = summary.lower()\n",
        "                text = text.lower()\n",
        "            if not text or not summary:\n",
        "                continue\n",
        "            temp.write(text + \"\\n\")\n",
        "            temp.write(summary + \"\\n\")\n",
        "    yttm.BPE.train(data=temp_file_name, vocab_size=vocab_size, model=model_path)\n",
        "\n",
        "train_bpe(train_records, \"BPE_model.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAkZ2f5LhWwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ae6202-fae8-464b-8f9a-3919ebb91797"
      },
      "source": [
        "bpe_processor = yttm.BPE('BPE_model.bin')\n",
        "bpe_processor.encode([\"октябрь богат на изменения\"], output_type=yttm.OutputType.SUBWORD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['▁октябрь', '▁богат', '▁на', '▁изменения']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOkUL_YIGp-S"
      },
      "source": [
        "## (!)\n",
        "Если надо, поменяйте код словаря"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhQYN1beiVEC"
      },
      "source": [
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, bpe_processor):\n",
        "        self.index2word = bpe_processor.vocab()\n",
        "        self.word2index = {w: i for i, w in enumerate(self.index2word)}\n",
        "        self.word2count = Counter()\n",
        "\n",
        "    def get_pad(self):\n",
        "        return self.word2index[\"<PAD>\"]\n",
        "\n",
        "    def get_sos(self):\n",
        "        return self.word2index[\"<SOS>\"]\n",
        "\n",
        "    def get_eos(self):\n",
        "        return self.word2index[\"<EOS>\"]\n",
        "\n",
        "    def get_unk(self):\n",
        "        return self.word2index[\"<UNK>\"]\n",
        "    \n",
        "    def has_word(self, word) -> bool:\n",
        "        return word in self.word2index\n",
        "\n",
        "    def get_index(self, word):\n",
        "        if word in self.word2index:\n",
        "            return self.word2index[word]\n",
        "        return self.get_unk()\n",
        "\n",
        "    def get_word(self, index):\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.index2word)\n",
        "\n",
        "    def is_empty(self):\n",
        "        empty_size = 4\n",
        "        return self.size() <= empty_size\n",
        "\n",
        "    def reset(self):\n",
        "        self.word2count = Counter()\n",
        "        self.index2word = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
        "        self.word2index = {word: index for index, word in enumerate(self.index2word)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qvZtNcOifAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44ca9bb-fcad-45cf-b10f-932c68a50773"
      },
      "source": [
        "vocabulary = Vocabulary(bpe_processor)\n",
        "vocabulary.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdb-39jO-72q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717ef108-b39f-4279-82c2-522d0aed5c48"
      },
      "source": [
        "from rouge import Rouge\n",
        "import razdel\n",
        "\n",
        "def add_oracle_summary_to_records(records, max_sentences=30, lower=True, nrows=1000):\n",
        "    rouge = Rouge()\n",
        "    sentences_ = []\n",
        "    oracle_sentences_ = []\n",
        "    oracle_summary_ = []\n",
        "    if nrows is not None:\n",
        "        records = records.iloc[:nrows].copy()\n",
        "    else:\n",
        "        records = records.copy()\n",
        "\n",
        "    for text, summary in tqdm(records[['text', 'summary']].values):\n",
        "        summary = summary.lower() if lower else summary\n",
        "        sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
        "        oracle_summary, sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge),\n",
        "                                                                         lower=lower, max_sentences=max_sentences)\n",
        "        sentences_ += [sentences]\n",
        "        oracle_sentences_ += [list(sentences_indicies)]\n",
        "        oracle_summary_ += [oracle_summary]\n",
        "    records['sentences'] = sentences_\n",
        "    records['oracle_sentences'] = oracle_sentences_\n",
        "    records['oracle_summary'] = oracle_summary_\n",
        "    return records\n",
        "\n",
        "ext_train_records = add_oracle_summary_to_records(train_records, nrows=30000)\n",
        "ext_val_records = add_oracle_summary_to_records(val_records, nrows=None)\n",
        "ext_test_records = add_oracle_summary_to_records(test_records, nrows=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [27:08<00:00, 18.43it/s]\n",
            "100%|██████████| 5265/5265 [05:26<00:00, 16.11it/s]\n",
            "100%|██████████| 5770/5770 [05:49<00:00, 16.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN0fU56U_21C"
      },
      "source": [
        "Используй `pickle` для сохранения записей, чтобы потом не пересоздавать их потом. Если решаешь задание в колабе, можешь подключить свой гугл диск и сохранить данные в нём."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btDziYEK_21C"
      },
      "source": [
        "import pickle\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "with open(\"train_records.bin\", 'wb') as file:\n",
        "    pickle.dump(ext_train_records, file)\n",
        "with open(\"val_records.bin\", 'wb') as file:\n",
        "    pickle.dump(ext_val_records, file)\n",
        "with open(\"test_records.bin\", 'wb') as file:\n",
        "    pickle.dump(ext_test_records, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlXXc8qUHC5m"
      },
      "source": [
        "## (!)\n",
        "Если надо, поменяйте код генератора датасета и батчевалки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNyxstTChK3C"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import razdel\n",
        "import torch\n",
        "import numpy as np\n",
        "from rouge import Rouge\n",
        "\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "\n",
        "class ExtDataset(data.Dataset):\n",
        "    def __init__(self, records, vocabulary, bpe_processor, lower=True, max_sentences=30, max_sentence_length=50, device=torch.device('cpu')):\n",
        "        self.records = records\n",
        "        self.num_samples = records.shape[0]\n",
        "        self.bpe_processor = bpe_processor\n",
        "        self.lower = lower\n",
        "        self.rouge = Rouge()\n",
        "        self.vocabulary = vocabulary\n",
        "        self.max_sentences = max_sentences\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "        self.device = device\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.records.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cur_record = self.records.iloc[idx]\n",
        "        inputs = list(map(lambda x: x[:self.max_sentence_length], self.bpe_processor.encode(cur_record['sentences'], output_type=yttm.OutputType.ID)))\n",
        "        outputs = [int(i in cur_record['oracle_sentences']) for i in range(len(cur_record['sentences']))]\n",
        "        return {'inputs': inputs, 'outputs': outputs}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvARjudojEDD"
      },
      "source": [
        "# Это батчевалка\n",
        "def collate_fn(records):\n",
        "    max_length = max(len(sentence) for record in records for sentence in record['inputs'])\n",
        "    max_sentences = max(len(record['outputs']) for record in records)\n",
        "\n",
        "    new_inputs = torch.zeros((len(records), max_sentences, max_length))\n",
        "    new_outputs = torch.zeros((len(records), max_sentences))\n",
        "    for i, record in enumerate(records):\n",
        "        for j, sentence in enumerate(record['inputs']):\n",
        "            new_inputs[i, j, :len(sentence)] += np.array(sentence)\n",
        "        new_outputs[i, :len(record['outputs'])] += np.array(record['outputs'])\n",
        "    return {'features': new_inputs.type(torch.LongTensor), 'targets': new_outputs}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OoWvn-r4rJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031c0fc3-b648-4f9a-d1b3-247970969ea1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "\n",
        "\n",
        "class YourSentenceEncoder(nn.Module):\n",
        "    # Место для вашего Sentence Encoder-а. Разрешается использовать любые методы, которые вам нравятся.\n",
        "    def __init__(self, input_size,\n",
        "                       embedding_dim, \n",
        "                       hidden_size,\n",
        "                       n_layers=3, \n",
        "                       dropout=0.3, \n",
        "                       bidirectional=True):\n",
        "        super().__init__()\n",
        "\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        hidden_size = hidden_size // num_directions\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "        self.embedding_layer = nn.Embedding(input_size, embedding_dim)\n",
        "        self.rnn_layer = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        embedded = self.embedding_layer(inputs)\n",
        "        outputs, _ = self.rnn_layer(embedded, hidden)\n",
        "        sentences_embeddings = torch.mean(outputs, 1)\n",
        "        return sentences_embeddings\n",
        "\n",
        "\n",
        "\n",
        "class SentenceTaggerRNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 token_embedding_dim=256,\n",
        "                 sentence_encoder_hidden_size=256,\n",
        "                 hidden_size=256,\n",
        "                 bidirectional=True,\n",
        "                 sentence_encoder_n_layers=2,\n",
        "                 sentence_encoder_dropout=0.3,\n",
        "                 sentence_encoder_bidirectional=True,\n",
        "                 n_layers=1,\n",
        "                 dropout=0.3):\n",
        "        super(SentenceTaggerRNN, self).__init__()\n",
        "\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        assert hidden_size % num_directions == 0\n",
        "        hidden_size = hidden_size // num_directions\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Your sentence encoder model\n",
        "        self.sentence_encoder = YourSentenceEncoder(vocabulary_size, token_embedding_dim,\n",
        "                                                   sentence_encoder_hidden_size, sentence_encoder_n_layers, \n",
        "                                                   sentence_encoder_dropout, sentence_encoder_bidirectional)\n",
        "        \n",
        "        self.rnn_layer = nn.LSTM(\n",
        "            sentence_encoder_hidden_size, \n",
        "            hidden_size, \n",
        "            n_layers, \n",
        "            dropout=dropout,\n",
        "            bidirectional=bidirectional, \n",
        "            batch_first=True)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n",
        "        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
        "        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
        "        self.tanh_layer = nn.Tanh()\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        batch_size = inputs.size(0)\n",
        "        sentences_count = inputs.size(1)\n",
        "        tokens_count = inputs.size(2)\n",
        "        inputs = inputs.reshape(-1, tokens_count)\n",
        "        embedded_sentences = self.sentence_encoder(inputs)\n",
        "        embedded_sentences = embedded_sentences.reshape(batch_size, sentences_count, -1)\n",
        "        outputs, _ = self.rnn_layer(embedded_sentences, hidden)\n",
        "        outputs = self.dropout_layer(outputs)\n",
        "        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n",
        "        content = self.content_linear_layer(outputs).squeeze(2)\n",
        "        salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2)\n",
        "        return content + salience\n",
        "\n",
        "model = SentenceTaggerRNN(vocabulary.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q2Gb6ODHHB_"
      },
      "source": [
        "### Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVDW8raJeQxn"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "loaders = {\n",
        "    'train': data.DataLoader(\n",
        "        ExtDataset(\n",
        "            ext_train_records, \n",
        "            vocabulary, \n",
        "            bpe_processor=bpe_processor\n",
        "        ), \n",
        "        batch_size=64, \n",
        "        collate_fn=collate_fn\n",
        "    ),\n",
        "    'valid': data.DataLoader(\n",
        "        ExtDataset(\n",
        "            ext_val_records, \n",
        "            vocabulary, \n",
        "            bpe_processor=bpe_processor\n",
        "        ), \n",
        "        batch_size=64, \n",
        "        collate_fn=collate_fn\n",
        "    ),\n",
        "    'test': data.DataLoader(\n",
        "        ExtDataset(\n",
        "            ext_test_records, \n",
        "            vocabulary, \n",
        "            bpe_processor=bpe_processor\n",
        "        ), \n",
        "        batch_size=64, \n",
        "        collate_fn=collate_fn\n",
        "    ),\n",
        "}\n",
        "\n",
        "lr = 3e-5\n",
        "num_epochs = 2\n",
        "\n",
        "optimizer  = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# Maybe adding scheduler?"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUe5fdJHx8Ez"
      },
      "source": [
        "from tqdm.notebook import tqdm, trange"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4JQuWVu_21D"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.to(device)\n",
        "    pbar_loader = trange(len(loaders[\"train\"]) + len(loaders[\"valid\"]), desc=f\"Train Loss: {0}, Valid Loss: {0}\")\n",
        "    for e in trange(num_epochs, desc=\"Epoch\"):\n",
        "        train_loss = 0\n",
        "        valid_loss = 0\n",
        "        train_it = 0\n",
        "        valid_it = 0\n",
        "        \n",
        "        model.train()\n",
        "        for batch in loaders[\"train\"]:\n",
        "            features = batch[\"features\"].to(device)\n",
        "            targets = batch[\"targets\"].to(device)\n",
        "            \n",
        "            logits = model(features)\n",
        "            \n",
        "            loss = criterion(logits, targets)\n",
        "            train_loss += loss.item()\n",
        "            train_it += 1\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Maybe adding scheduler?\n",
        "            \n",
        "            pbar_loader.update()\n",
        "            pbar_loader.set_description(\n",
        "                f\"Train Loss: {train_loss / train_it:.3}, Valid Loss: {0}\"\n",
        "            )\n",
        "            \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in loaders[\"valid\"]:\n",
        "                features = batch[\"features\"].to(device)\n",
        "                targets = batch[\"targets\"].to(device)\n",
        "\n",
        "                logits = model(features)\n",
        "\n",
        "                loss = criterion(logits, targets)\n",
        "                valid_loss += loss.item()\n",
        "                valid_it += 1\n",
        "                \n",
        "                pbar_loader.update()\n",
        "                pbar_loader.set_description(\n",
        "                    f\"Train Loss: {train_loss / train_it:.3},\"\n",
        "                    f\" Valid Loss: {valid_loss / valid_it:.3}\"\n",
        "                )\n",
        "        print(\n",
        "            f\"Epoch {e}; Train Loss: {train_loss / train_it:.3},\"\n",
        "            f\" Valid Loss: {valid_loss / valid_it:.3}\"\n",
        "        )\n",
        "        pbar_loader.reset()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08rh-ORT_21D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132,
          "referenced_widgets": [
            "b3b2853280ff4718ae63723c27347c76",
            "5cc33efa05894ec88be9bd471188e69d",
            "4ffe091214564bc183747f62e2effeb4",
            "bd921d4a30ce419c80d8f41557f0bc03",
            "30b850e3c0e043a59b4ce8928a1363a1",
            "1287e4d6cfc546fcab3b3175dbcc9ee6",
            "03cde1d8acbf452591ef3f4227420c47",
            "c4ee89ee09434de496f31d4b028b166a",
            "e26339fb20d04ca8961fc85487525efd",
            "9bb50fbb67274c34ac5d99dc8677b84a",
            "e6e661e67f86460989add87e1496b586",
            "6f8b68b5bcf94ab3883a3886e6bf0e9a",
            "08f8e13637584881b3f43d7fc287b5cc",
            "ed8c001eb0fe46aeaf3762b99ad415e4",
            "ac590b88ba1d46d6938542d448420ab2",
            "456b8b1ea6da446a9f8e3c9d8f8e0962"
          ]
        },
        "outputId": "e409944d-a622-426a-8e78-fad909a69e67"
      },
      "source": [
        "train()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3b2853280ff4718ae63723c27347c76",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Train Loss: 0, Valid Loss: 0', max=552.0, style=ProgressS…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e26339fb20d04ca8961fc85487525efd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0; Train Loss: 0.183, Valid Loss: 0.179\n",
            "Epoch 1; Train Loss: 0.182, Valid Loss: 0.178\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kc0etEGfJ0p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "c4c68f466bb245b2a2c7fded0c84c722",
            "5cdb9298c6a44326bb6026a7ac74edf1",
            "33b4d29d715d45afaf342b8c12c2c092",
            "5d6996bd96a244e386d23d646b9f71d4",
            "5dbb21716336492ebb7756cfbb27827e",
            "47a2f3c7fbe14d11ab7339f8bb6e6c8e",
            "93f6147a5563491993fd8f46a7ac3fb5",
            "b7c503b680bc4206862672041d6a0872"
          ]
        },
        "outputId": "76328826-a17b-448c-e960-26c80b5d3d7c"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "top_k = 3\n",
        "references = []\n",
        "predictions = []\n",
        "\n",
        "def postprocess(ref, hyp, is_multiple_ref=False, detokenize_after=False, tokenize_after=True):\n",
        "    if is_multiple_ref:\n",
        "        reference_sents = ref.split(\" s_s \")\n",
        "        decoded_sents = hyp.split(\"s_s\")\n",
        "        hyp = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in decoded_sents]\n",
        "        ref = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in reference_sents]\n",
        "        hyp = \" \".join(hyp)\n",
        "        ref = \" \".join(ref)\n",
        "    ref = ref.strip()\n",
        "    hyp = hyp.strip()\n",
        "    if detokenize_after:\n",
        "        hyp = punct_detokenize(hyp)\n",
        "        ref = punct_detokenize(ref)\n",
        "    if tokenize_after:\n",
        "        hyp = hyp.replace(\"@@UNKNOWN@@\", \"<unk>\")\n",
        "        hyp = \" \".join([token.text for token in razdel.tokenize(hyp)])\n",
        "        ref = \" \".join([token.text for token in razdel.tokenize(ref)])\n",
        "    return ref, hyp\n",
        "\n",
        "model.eval()\n",
        "for num, batch in tqdm(enumerate(loaders[\"test\"]), total = len(loaders[\"test\"]),leave=False):\n",
        "\n",
        "    logits = model(batch[\"features\"].to(device))\n",
        "    in_summary = torch.argsort(logits, dim=1)[:, -top_k:]\n",
        "    for i in range(len(batch['targets'])):\n",
        "\n",
        "        summary = ext_test_records.iloc[i]['summary']\n",
        "        summary = summary.lower()\n",
        "        predicted_summary = ' '.join([ext_test_records.iloc[i]['sentences'][idx] for idx in in_summary[i].sort()[0] if idx < len(ext_test_records.iloc[i]['sentences'])])\n",
        "        summary, predicted_summary = postprocess(summary, predicted_summary)\n",
        "\n",
        "        references.append(summary)\n",
        "        predictions.append(predicted_summary)\n",
        "\n",
        "calc_scores(references, predictions)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4c68f466bb245b2a2c7fded0c84c722",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=91.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Count: 5770\n",
            "Ref: болельщики « манчестер юнайтед » атаковали дом вице-президента клуба эда вудворда . « красные дьяволы » намерены пожизненно отстранить от посещения домашних матчей участников нападения . не так давно фанаты желали функционеру смерти , а на следующем матче команды планируют провести акцию протеста против политики исполнительного директора . сам вудворд может покинуть « мю » из-за нарастающей агрессии в его адрес .\n",
            "Hyp: активная часть болельщиков « манчестер юнайтед » , выражающая недовольство политикой вице-президента клуба эда вудворда , продолжает нападки на руководство . в ходе матча 1/16 финала кубка англии против « транмир роверс » ( 6 : 0 ) фанаты манкунианцев заряжали скандирования « вудворд умрет » , а спустя несколько дней перешли от слов к действиям . как стало известно sky sports , в ночь на 29 января поклонники « красных дьяволов » совершили нападение на дом исполнительного директора .\n",
            "BLEU:  0.4432270997283035\n",
            "ROUGE:  {'rouge-1': {'f': 0.3075880828228737, 'p': 0.3011957219334286, 'r': 0.3369665141700279}, 'rouge-2': {'f': 0.14171527105032555, 'p': 0.13715317270922162, 'r': 0.1592469145621744}, 'rouge-l': {'f': 0.2601112937116828, 'p': 0.2695295192599504, 'r': 0.30085689509473823}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H_B3T_TxXrd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}